{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc272c7a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-11-23T08:02:49.216284Z",
     "iopub.status.busy": "2023-11-23T08:02:49.215920Z",
     "iopub.status.idle": "2023-11-23T08:02:53.721049Z",
     "shell.execute_reply": "2023-11-23T08:02:53.720072Z"
    },
    "papermill": {
     "duration": 4.512294,
     "end_time": "2023-11-23T08:02:53.723647",
     "exception": false,
     "start_time": "2023-11-23T08:02:49.211353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet18\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9ec747b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T08:02:53.730851Z",
     "iopub.status.busy": "2023-11-23T08:02:53.730310Z",
     "iopub.status.idle": "2023-11-23T08:02:53.735126Z",
     "shell.execute_reply": "2023-11-23T08:02:53.734263Z"
    },
    "papermill": {
     "duration": 0.010422,
     "end_time": "2023-11-23T08:02:53.737139",
     "exception": false,
     "start_time": "2023-11-23T08:02:53.726717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# It's really important to add an accelerator to your notebook, as otherwise the submission will fail.\n",
    "# We recomment using the P100 GPU rather than T4 as it's faster and will increase the chances of passing the time cut-off threshold.\n",
    "\n",
    "if DEVICE != 'cuda':\n",
    "    raise RuntimeError('Make sure you have added an accelerator to your notebook; the submission will fail otherwise!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "483112cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T08:02:53.743498Z",
     "iopub.status.busy": "2023-11-23T08:02:53.743190Z",
     "iopub.status.idle": "2023-11-23T08:02:53.755220Z",
     "shell.execute_reply": "2023-11-23T08:02:53.754546Z"
    },
    "papermill": {
     "duration": 0.017338,
     "end_time": "2023-11-23T08:02:53.757101",
     "exception": false,
     "start_time": "2023-11-23T08:02:53.739763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper functions for loading the hidden dataset.\n",
    "\n",
    "def load_example(df_row):\n",
    "    image = torchvision.io.read_image(df_row['image_path'])\n",
    "    result = {\n",
    "        'image': image,\n",
    "        'image_id': df_row['image_id'],\n",
    "        'age_group': df_row['age_group'],\n",
    "        'age': df_row['age'],\n",
    "        'person_id': df_row['person_id']\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "class HiddenDataset(Dataset):\n",
    "    '''The hidden dataset.'''\n",
    "    def __init__(self, split='train'):\n",
    "        super().__init__()\n",
    "        self.examples = []\n",
    "\n",
    "        df = pd.read_csv(f'/kaggle/input/neurips-2023-machine-unlearning/{split}.csv')\n",
    "        df['image_path'] = df['image_id'].apply(\n",
    "            lambda x: os.path.join('/kaggle/input/neurips-2023-machine-unlearning/', 'images', x.split('-')[0], x.split('-')[1] + '.png'))\n",
    "        df = df.sort_values(by='image_path')\n",
    "        df.apply(lambda row: self.examples.append(load_example(row)), axis=1)\n",
    "        if len(self.examples) == 0:\n",
    "            raise ValueError('No examples.')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        image = example['image']\n",
    "        image = image.to(torch.float32)\n",
    "        example['image'] = image\n",
    "        return example\n",
    "\n",
    "\n",
    "def get_dataset(batch_size):\n",
    "    '''Get the dataset.'''\n",
    "    retain_ds = HiddenDataset(split='retain')\n",
    "    forget_ds = HiddenDataset(split='forget')\n",
    "    val_ds = HiddenDataset(split='validation')\n",
    "\n",
    "    retain_loader = DataLoader(retain_ds, batch_size=batch_size, shuffle=True)\n",
    "    forget_loader = DataLoader(forget_ds, batch_size=batch_size, shuffle=True)\n",
    "    validation_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return retain_loader, forget_loader, validation_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "847559b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T08:02:53.763561Z",
     "iopub.status.busy": "2023-11-23T08:02:53.763097Z",
     "iopub.status.idle": "2023-11-23T08:02:53.768463Z",
     "shell.execute_reply": "2023-11-23T08:02:53.767632Z"
    },
    "papermill": {
     "duration": 0.010645,
     "end_time": "2023-11-23T08:02:53.770328",
     "exception": false,
     "start_time": "2023-11-23T08:02:53.759683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def random_mask_params(net, reset_param_names, ratio=0.2):\n",
    "    net_params = {n: p for n, p in net.named_parameters()}\n",
    "    for name in reset_param_names:\n",
    "        mask = (torch.rand_like(net_params[name].data) > ratio).type(torch.float32)   \n",
    "        if 'bn' not in name:\n",
    "            net_params[name].data = mask * net_params[name].data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6da581f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T08:02:53.776881Z",
     "iopub.status.busy": "2023-11-23T08:02:53.776431Z",
     "iopub.status.idle": "2023-11-23T08:02:53.786031Z",
     "shell.execute_reply": "2023-11-23T08:02:53.785209Z"
    },
    "papermill": {
     "duration": 0.014852,
     "end_time": "2023-11-23T08:02:53.787936",
     "exception": false,
     "start_time": "2023-11-23T08:02:53.773084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can replace the below simple unlearning with your own unlearning function.\n",
    "\n",
    "def unlearning(\n",
    "    net, \n",
    "    retain_loader, \n",
    "    forget_loader, \n",
    "    val_loader, param_names):\n",
    "    \"\"\"Simple unlearning by finetuning.\"\"\"\n",
    "    epochs = 5\n",
    "    lr = 0.0001\n",
    "    lr_ratio = 1\n",
    "    ratio = 0.5\n",
    "    reset_param_names = [name for name in param_names if 'bn' not in name]\n",
    "    random_mask_params(net, reset_param_names, ratio=ratio)\n",
    "    param_group1 = [param for name, param in net.named_parameters() if name in reset_param_names]\n",
    "    param_group2 = [param for name, param in net.named_parameters() if name not in reset_param_names]    \n",
    "    params = [\n",
    "        {'params': param_group1, 'lr': lr * lr_ratio},\n",
    "        {'params': param_group2}\n",
    "    ]    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(params, lr=lr, weight_decay=1e-4)    \n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=epochs * len(retain_loader), eta_min=lr*0.1)\n",
    "    net.train()\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        net.train()\n",
    "        for sample in retain_loader:\n",
    "            inputs = sample[\"image\"]\n",
    "            targets = sample[\"age_group\"]\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "    net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1ed773a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T08:02:53.796238Z",
     "iopub.status.busy": "2023-11-23T08:02:53.795695Z",
     "iopub.status.idle": "2023-11-23T08:02:53.811057Z",
     "shell.execute_reply": "2023-11-23T08:02:53.810065Z"
    },
    "papermill": {
     "duration": 0.022554,
     "end_time": "2023-11-23T08:02:53.813108",
     "exception": false,
     "start_time": "2023-11-23T08:02:53.790554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists('/kaggle/input/neurips-2023-machine-unlearning/empty.txt'):\n",
    "    # mock submission\n",
    "    subprocess.run('touch submission.zip', shell=True)\n",
    "else:\n",
    "    \n",
    "    # Note: it's really important to create the unlearned checkpoints outside of the working directory \n",
    "    # as otherwise this notebook may fail due to running out of disk space.\n",
    "    # The below code saves them in /kaggle/tmp to avoid that issue.\n",
    "    \n",
    "    os.makedirs('/kaggle/tmp', exist_ok=True)\n",
    "    retain_loader, forget_loader, validation_loader = get_dataset(64)\n",
    "    net = resnet18(weights=None, num_classes=10)\n",
    "    net.to(DEVICE)\n",
    "    \n",
    "    param_names = [n for n, _ in net.named_parameters()]    \n",
    "    for i in range(512):\n",
    "        net.load_state_dict(torch.load('/kaggle/input/neurips-2023-machine-unlearning/original_model.pth'))\n",
    "        unlearning(net, retain_loader, forget_loader, validation_loader, param_names)\n",
    "        state = net.state_dict()\n",
    "        torch.save(state, f'/kaggle/tmp/unlearned_checkpoint_{i}.pth')\n",
    "        \n",
    "    # Ensure that submission.zip will contain exactly 512 checkpoints \n",
    "    # (if this is not the case, an exception will be thrown).\n",
    "    unlearned_ckpts = os.listdir('/kaggle/tmp')\n",
    "    if len(unlearned_ckpts) != 512:\n",
    "        raise RuntimeError('Expected exactly 512 checkpoints. The submission will throw an exception otherwise.')\n",
    "        \n",
    "    subprocess.run('zip submission.zip /kaggle/tmp/*.pth', shell=True)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 6535361,
     "sourceId": 56167,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30554,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9.375438,
   "end_time": "2023-11-23T08:02:55.137396",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-23T08:02:45.761958",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
